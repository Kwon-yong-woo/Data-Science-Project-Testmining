{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "transformers.__version__\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                           s[\"pos\"].values.tolist(),\n",
    "                                                           s[\"NER\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>rule_label</th>\n",
       "      <th>IOB</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enter</th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Number</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Number</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Number</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  word rule_label IOB  sentence  enter  review          pos NER\n",
       "0           0  2020        NaN   O         0      0       0       Number   O\n",
       "1           1     .        NaN   O         0      0       0  Punctuation   O\n",
       "2           2    08        NaN   O         0      0       0       Number   O\n",
       "3           3     .        NaN   O         0      0       0  Punctuation   O\n",
       "4           4    07        NaN   O         0      0       0       Number   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ = r\"C:\\Users\\LAB\\jupyter\\DeepLearning_project\\data\\NER\\{}.xlsx\"\n",
    "\n",
    "buds_live = pd.read_excel(path_.format(\"buds_live\"), encoding=\"UTF-8\").fillna(method=\"ffill\")\n",
    "buds_live.word=buds_live.word.astype(str)\n",
    "\n",
    "buds_live.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020', 'Number', 'O'),\n",
       " ('.', 'Punctuation', 'O'),\n",
       " ('08', 'Number', 'O'),\n",
       " ('.', 'Punctuation', 'O'),\n",
       " ('07', 'Number', 'O'),\n",
       " ('로켓', 'Noun', 'O'),\n",
       " ('와우', 'Noun', 'O'),\n",
       " ('아침', 'Noun', 'O'),\n",
       " ('6시', 'Number', 'O'),\n",
       " ('에', 'Foreign', 'O'),\n",
       " ('수령', 'Noun', 'O'),\n",
       " ('받다', 'Verb', 'O'),\n",
       " ('뒤', 'Noun', 'O'),\n",
       " ('페어', 'Noun', 'O'),\n",
       " ('링후', 'Noun', 'O'),\n",
       " ('약', 'Noun', 'O'),\n",
       " ('15분', 'Number', 'O'),\n",
       " ('간', 'Foreign', 'O'),\n",
       " ('사용', 'Noun', 'O'),\n",
       " ('후', 'Noun', 'O'),\n",
       " ('후기', 'Noun', 'O'),\n",
       " ('올리다', 'Verb', 'O'),\n",
       " ('.', 'Punctuation', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getter = SentenceGetter(buds_live)\n",
    "getter.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020',\n",
       " '.',\n",
       " '08',\n",
       " '.',\n",
       " '07',\n",
       " '로켓',\n",
       " '와우',\n",
       " '아침',\n",
       " '6시',\n",
       " '에',\n",
       " '수령',\n",
       " '받다',\n",
       " '뒤',\n",
       " '페어',\n",
       " '링후',\n",
       " '약',\n",
       " '15분',\n",
       " '간',\n",
       " '사용',\n",
       " '후',\n",
       " '후기',\n",
       " '올리다',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = list(set(buds_live[\"NER\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 70\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0eb9bf8310490cb01dd78fa28070f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf19f6227cb4ca6a0ecd4004b351941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c14d9b337064516b0cf39d89e2a1c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for labels_ in labels:\n",
    "    for label_ in labels_:\n",
    "        if type(label_) == \"NoneType\":\n",
    "            print(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs).to(torch.int64)\n",
    "val_inputs = torch.tensor(val_inputs).to(torch.int64)\n",
    "tr_tags = torch.tensor(tr_tags).to(torch.int64)\n",
    "val_tags = torch.tensor(val_tags).to(torch.int64)\n",
    "tr_masks = torch.tensor(tr_masks).to(torch.int64)\n",
    "val_masks = torch.tensor(val_masks).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329ebd8c6879481cbf6dfa1634c3c115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db17424169694bdeaa60e905801b870b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                    | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.6135273851266428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|██▎                                                                    | 1/30 [25:09<12:09:21, 1509.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6627909583704812\n",
      "Validation Accuracy: 0.8733962720890825\n",
      "Validation F1-Score: 0.02185792349726776\n",
      "\n",
      "Average train loss: 0.6090371934305719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|████▋                                                                  | 2/30 [46:51<10:47:31, 1387.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6677754925830024\n",
      "Validation Accuracy: 0.8748487049140644\n",
      "Validation F1-Score: 0.02203856749311295\n",
      "\n",
      "Average train loss: 0.6043281798102275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|███████                                                               | 3/30 [1:03:49<9:08:33, 1219.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6690826139279774\n",
      "Validation Accuracy: 0.8755749213265553\n",
      "Validation F1-Score: 0.027624309392265192\n",
      "\n",
      "Average train loss: 0.5966842505110412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█████████▎                                                            | 4/30 [1:20:46<8:13:36, 1139.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6730221148048129\n",
      "Validation Accuracy: 0.8760590656015492\n",
      "Validation F1-Score: 0.033426183844011144\n",
      "\n",
      "Average train loss: 0.5877877809420353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|███████████▋                                                          | 5/30 [1:37:49<7:37:14, 1097.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6749592444726399\n",
      "Validation Accuracy: 0.8743645606390704\n",
      "Validation F1-Score: 0.032\n",
      "\n",
      "Average train loss: 0.5782547906166365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██████████████                                                        | 6/30 [1:54:50<7:08:28, 1071.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6662779769727162\n",
      "Validation Accuracy: 0.8743645606390704\n",
      "Validation F1-Score: 0.03183023872679045\n",
      "\n",
      "Average train loss: 0.57117240393863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|████████████████▎                                                     | 7/30 [2:11:55<6:44:50, 1056.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.668658903666905\n",
      "Validation Accuracy: 0.8753328491890583\n",
      "Validation F1-Score: 0.032171581769437\n",
      "\n",
      "Average train loss: 0.5631743904923191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██████████████████▋                                                   | 8/30 [2:30:39<6:35:08, 1077.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6755708754062653\n",
      "Validation Accuracy: 0.8755749213265553\n",
      "Validation F1-Score: 0.032171581769437\n",
      "\n",
      "Average train loss: 0.5544634761930514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|█████████████████████                                                 | 9/30 [2:48:06<6:13:50, 1068.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.678057889853205\n",
      "Validation Accuracy: 0.8758169934640523\n",
      "Validation F1-Score: 0.03252032520325203\n",
      "\n",
      "Average train loss: 0.5480209992212408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███████████████████████                                              | 10/30 [3:05:25<5:53:01, 1059.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.672495920743261\n",
      "Validation Accuracy: 0.8736383442265795\n",
      "Validation F1-Score: 0.04712041884816754\n",
      "\n",
      "Average train loss: 0.5401341088679659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|█████████████████████████▎                                           | 11/30 [3:22:45<5:33:31, 1053.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6714408376387188\n",
      "Validation Accuracy: 0.8753328491890583\n",
      "Validation F1-Score: 0.0427807486631016\n",
      "\n",
      "Average train loss: 0.5344989009264136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|███████████████████████████▌                                         | 12/30 [3:40:10<5:15:15, 1050.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6781922876834869\n",
      "Validation Accuracy: 0.8724279835390947\n",
      "Validation F1-Score: 0.046153846153846156\n",
      "\n",
      "Average train loss: 0.5280968511805815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|█████████████████████████████▉                                       | 13/30 [3:57:45<4:58:06, 1052.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6833847377981458\n",
      "Validation Accuracy: 0.8712176228516098\n",
      "Validation F1-Score: 0.03655352480417755\n",
      "\n",
      "Average train loss: 0.5198602839177396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████████████████████████████████▏                                    | 14/30 [4:15:04<4:39:31, 1048.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6797747101102557\n",
      "Validation Accuracy: 0.8733962720890825\n",
      "Validation F1-Score: 0.04651162790697675\n",
      "\n",
      "Average train loss: 0.5175337676240617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|██████████████████████████████████▌                                  | 15/30 [4:32:30<4:21:50, 1047.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.696170877133097\n",
      "Validation Accuracy: 0.86879690147664\n",
      "Validation F1-Score: 0.04081632653061224\n",
      "\n",
      "Average train loss: 0.5079863695036463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|████████████████████████████████████▊                                | 16/30 [4:49:53<4:04:06, 1046.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7028906430516925\n",
      "Validation Accuracy: 0.8714596949891068\n",
      "Validation F1-Score: 0.03664921465968587\n",
      "\n",
      "Average train loss: 0.502802061183112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|███████████████████████████████████████                              | 17/30 [5:06:57<3:45:12, 1039.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7060786890132087\n",
      "Validation Accuracy: 0.8625030259017187\n",
      "Validation F1-Score: 0.04694835680751174\n",
      "\n",
      "Average train loss: 0.4955678637288198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████▍                           | 18/30 [5:24:02<3:27:01, 1035.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7113442676407951\n",
      "Validation Accuracy: 0.8649237472766884\n",
      "Validation F1-Score: 0.03931203931203931\n",
      "\n",
      "Average train loss: 0.4905479202250473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|███████████████████████████████████████████▋                         | 19/30 [5:41:14<3:09:35, 1034.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7106467847313199\n",
      "Validation Accuracy: 0.8678286129266521\n",
      "Validation F1-Score: 0.034999999999999996\n",
      "\n",
      "Average train loss: 0.4862375667616099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████████████████████████████████████████████                       | 20/30 [5:58:19<2:51:53, 1031.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7202487311192921\n",
      "Validation Accuracy: 0.8654078915516824\n",
      "Validation F1-Score: 0.04761904761904762\n",
      "\n",
      "Average train loss: 0.48282389004691306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|████████████████████████████████████████████████▎                    | 21/30 [6:15:21<2:34:17, 1028.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7180486129862922\n",
      "Validation Accuracy: 0.8656499636891793\n",
      "Validation F1-Score: 0.04842615012106538\n",
      "\n",
      "Average train loss: 0.47683957319299713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|██████████████████████████████████████████████████▌                  | 22/30 [6:32:23<2:16:54, 1026.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7137645993913923\n",
      "Validation Accuracy: 0.8685548293391431\n",
      "Validation F1-Score: 0.04975124378109453\n",
      "\n",
      "Average train loss: 0.4748135438235868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|████████████████████████████████████████████████████▉                | 23/30 [6:49:14<1:59:13, 1021.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7224998878581184\n",
      "Validation Accuracy: 0.8651658194141855\n",
      "Validation F1-Score: 0.05755395683453237\n",
      "\n",
      "Average train loss: 0.4707137635525535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|███████████████████████████████████████████████████████▏             | 24/30 [7:06:01<1:41:43, 1017.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7238314322062901\n",
      "Validation Accuracy: 0.8632292423142096\n",
      "Validation F1-Score: 0.047058823529411764\n",
      "\n",
      "Average train loss: 0.46636751820059386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|█████████████████████████████████████████████████████████▌           | 25/30 [7:23:02<1:24:53, 1018.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7271541442189898\n",
      "Validation Accuracy: 0.8646816751391915\n",
      "Validation F1-Score: 0.043165467625899276\n",
      "\n",
      "Average train loss: 0.4616482225285859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|███████████████████████████████████████████████████████████▊         | 26/30 [7:40:02<1:07:55, 1018.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7342226483992168\n",
      "Validation Accuracy: 0.8641975308641975\n",
      "Validation F1-Score: 0.047393364928909956\n",
      "\n",
      "Average train loss: 0.4589640603345983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|███████████████████████████████████████████████████████████████▉       | 27/30 [7:56:59<50:55, 1018.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7358960807323456\n",
      "Validation Accuracy: 0.8632292423142096\n",
      "Validation F1-Score: 0.047505938242280284\n",
      "\n",
      "Average train loss: 0.4557717129462907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|██████████████████████████████████████████████████████████████████▎    | 28/30 [8:14:02<33:59, 1019.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7396984483514514\n",
      "Validation Accuracy: 0.8629871701767127\n",
      "Validation F1-Score: 0.04716981132075472\n",
      "\n",
      "Average train loss: 0.45522958516072826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|████████████████████████████████████████████████████████████████████▋  | 29/30 [8:31:10<17:02, 1022.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7447442476238523\n",
      "Validation Accuracy: 0.8617768094892277\n",
      "Validation F1-Score: 0.046403712296983764\n",
      "\n",
      "Average train loss: 0.4534198239821346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████| 30/30 [8:49:29<00:00, 1058.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7429871473993573\n",
      "Validation Accuracy: 0.8637133865892036\n",
      "Validation F1-Score: 0.047393364928909956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    # 각 epoch마다 Reset.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # parameter 업데이트\n",
    "        optimizer.step()\n",
    "        # learning rate 업데이트\n",
    "        scheduler.step()\n",
    "\n",
    "    # 평균 loss 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # loss value 저장\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    # evaluation mode로 전환\n",
    "    model.eval()\n",
    "    # Reset epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # 계산할 모델이 있거나, Gredient 의 상황 알리기,\n",
    "        with torch.no_grad():\n",
    "            # Forward pass에는 logit predictions.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        #  CPU로 이동\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # test sentences accuracy .\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score([pred_tags], [valid_tags])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
